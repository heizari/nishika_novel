{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c970b0d0",
   "metadata": {},
   "source": [
    "## keywordのコサイン類似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf5c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_num = '014'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d1decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ginza\n",
    "import spacy\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import BertJapaneseTokenizer\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeZenkakuSpace(df, cols):\n",
    "    for col in cols:\n",
    "        df[col] = df[col].str.replace('　', ' ')\n",
    "    return df\n",
    "\n",
    "def histEachFavs(df, col, height=3, aspect=4, title=''):\n",
    "    '''\n",
    "    FAVごとのカラムの要素のヒストグラム\n",
    "    '''\n",
    "    for fav in sorted(df[FAV].unique()):\n",
    "        sns.catplot(x=col,data=df.query(f\"{FAV} == @fav\"),kind='count',height=height, aspect=aspect)\n",
    "        plt.title(f'{title} fav group:{fav}')\n",
    "        \n",
    "def getQueries(df, cols):\n",
    "    assert type(cols) == str or type(cols) == list, 'cols is str or list'\n",
    "    queries = []\n",
    "    if type(cols) == str:\n",
    "        queries = [f'{cols} == {flag}' for flag in sorted(df[cols].unique())]\n",
    "    elif type(cols) == list:\n",
    "        for col in cols:\n",
    "            col_queries = [f'{col} == {flag}' for flag in sorted(df[col].unique())]\n",
    "            if len(queries) == 0:\n",
    "                queries = col_queries\n",
    "            else:\n",
    "                queries = [f\"{q} & {col_queries[0]}\" for q in queries] + [f'{q} & {col_queries[1]}' for q in queries]\n",
    "            \n",
    "    return queries\n",
    "\n",
    "def columnUnique(df, col):\n",
    "    return sorted(df[col].unique())\n",
    "\n",
    "def histColumnsFavs(df, cols:list, height=3, aspect=6, title=''):\n",
    "    '''\n",
    "    カラムの要素に対するのFAVの数のヒストグラム\n",
    "    colsがlistのときは複数カラムの要素ごと\n",
    "    '''\n",
    "    queries = getQueries(df, cols)\n",
    "    for query in queries:\n",
    "        data = df.query(query)\n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "        sns.catplot(x=FAV,data=data,kind='count',height=height, aspect=aspect)\n",
    "        plt.title(f'{title} {query}')\n",
    "        \n",
    "def histEachFavsByCategory(df, df_target, col):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    cats = sorted(df[col].unique())\n",
    "    for cat in cats:\n",
    "        fig, axes = plt.subplots(figsize=(30,10), ncols=3, nrows=2)\n",
    "        for ax, fav in zip(axes.ravel(), favs):\n",
    "            target_mask = (df[FAV] == fav) & (df[col] == cat)\n",
    "            ax.hist(df_target.loc[target_mask], bins=100)\n",
    "            ax.set_xlim(-1,df_target.max())\n",
    "            ax.set_title(f'category:{cat}, fav:{fav}, num={target_mask.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b81b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '../output'\n",
    "train_df = pd.read_csv('../dataset/train.csv')\n",
    "test_df = pd.read_csv('../dataset/test.csv')\n",
    "ID = 'ncode'\n",
    "FAV = 'fav_novel_cnt_bin'\n",
    "favs = sorted(train_df[FAV].unique())\n",
    "\n",
    "train_df.userid = train_df.userid.astype('str')\n",
    "train_df.genre = train_df.genre.astype('str')\n",
    "test_df.userid = test_df.userid.astype('str')\n",
    "test_df.genre = test_df.genre.astype('str')\n",
    "str_cols = ['title', 'story', 'keyword', 'writer']\n",
    "train_df = removeZenkakuSpace(train_df, str_cols)\n",
    "test_df = removeZenkakuSpace(test_df, str_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a501f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=FAV, data=train_df, kind='count', height=3,aspect=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a73605",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'nlp' not in locals():\n",
    "    nlp = spacy.load(\"ja_ginza\")\n",
    "    print('load ginza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30662ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_no_keyword = ~train_df.keyword.isnull()\n",
    "train_df.keyword = train_df.keyword.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b756907",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSequenceVectorizer:\n",
    "    def __init__(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(self.model_name)\n",
    "        self.bert_model = transformers.BertModel.from_pretrained(self.model_name)\n",
    "        self.bert_model = self.bert_model.to(self.device)\n",
    "        self.max_len = 128\n",
    "            \n",
    "\n",
    "    def vectorize(self, sentence : str) -> np.array:\n",
    "        inp = self.tokenizer.encode(sentence)\n",
    "        len_inp = len(inp)\n",
    "\n",
    "        if len_inp >= self.max_len:\n",
    "            inputs = inp[:self.max_len]\n",
    "            masks = [1] * self.max_len\n",
    "        else:\n",
    "            inputs = inp + [0] * (self.max_len - len_inp)\n",
    "            masks = [1] * len_inp + [0] * (self.max_len - len_inp)\n",
    "\n",
    "        inputs_tensor = torch.tensor([inputs], dtype=torch.long).to(self.device)\n",
    "        masks_tensor = torch.tensor([masks], dtype=torch.long).to(self.device)\n",
    "        \n",
    "        seq_out = self.bert_model(inputs_tensor, masks_tensor)[0]\n",
    "        pooled_out = self.bert_model(inputs_tensor, masks_tensor)[1]\n",
    "\n",
    "        if torch.cuda.is_available():    \n",
    "            return seq_out[0][0].cpu().detach().numpy() # 0番目は [CLS] token, 768 dim の文章特徴量\n",
    "        else:\n",
    "            return seq_out[0][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa64a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applySpacyToKeyword(keywords):\n",
    "    return np.vectorize(lambda keyword: nlp(str(keyword)))(keywords)\n",
    "\n",
    "def applySpacyToRow(row):\n",
    "    keywords = row.split()\n",
    "    global pbar\n",
    "    pbar.update(1)\n",
    "    return BSV.vectorize(keywords)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6552caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[:3].loc[mask_no_keyword]['keyword_vectorize'] = applySpacyToRow(train_df[:3].loc[mask_no_keyword].keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa52ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSV = BertSequenceVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f0a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[:3].loc[mask_no_keyword]['keyword_vectorize'] = applySpacyToRow(train_df[:3].loc[mask_no_keyword].keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0451590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[:3].loc[mask_no_keyword]['vectorize'] = np.vectorize(lambda x : BSV.vectorize(x))(train_df[:3].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83912f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=sum(mask_no_keyword)) as pbar:\n",
    "    train_df.loc[mask_no_keyword]['keywords_vectrize'] = np.vectorize(applySpacyToRow)(train_df.loc[mask_no_keyword]['keyword'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b73b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['keywords'] = train_df.keyword.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeaa491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f297df",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vectorize(test, excluded='b')(a=train_df[:2]['writer'], c=train_df[:2]['genre'],b=train_df[:2][['ncode','writer']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
